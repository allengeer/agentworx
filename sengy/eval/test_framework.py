#!/usr/bin/env python3
"""
Test script to verify the evaluation framework is properly set up.
"""

import os
import sys

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


def test_imports():
    """Test that all required modules can be imported."""
    print("üß™ Testing Framework Imports...")
    
    try:
        from datasets import ENGINEERING_JIRA_TICKETS, ENGINEERING_MANAGER_SCENARIOS
        print("‚úÖ Datasets import successful")
    except ImportError as e:
        print(f"‚ùå Datasets import failed: {e}")
        return False
    
    try:
        from sengy.config import get_settings, validate_configuration
        print("‚úÖ Configuration import successful")
    except ImportError as e:
        print(f"‚ùå Configuration import failed: {e}")
        return False
    
    try:
        from langsmith_evaluator import SengyEvaluator
        print("‚úÖ Evaluator import successful")
    except ImportError as e:
        print(f"‚ùå Evaluator import failed: {e}")
        return False
    
    return True


def test_configuration():
    """Test configuration system."""
    print("\nüîß Testing Configuration System...")
    
    try:
        from sengy.config import get_settings
        settings = get_settings()
        
        print(f"‚úÖ Settings loaded: {settings.app_name} v{settings.version}")
        print(f"   Model: {settings.openai.model}")
        print(f"   Environment: {settings.environment}")
        
        return True
    except Exception as e:
        print(f"‚ùå Configuration test failed: {e}")
        return False


def test_datasets():
    """Test evaluation datasets."""
    print("\nüìä Testing Evaluation Datasets...")
    
    try:
        from datasets import ENGINEERING_MANAGER_SCENARIOS, SOFTWARE_ENGINEER_SCENARIOS
        
        print(f"‚úÖ Engineering Manager scenarios: {len(ENGINEERING_MANAGER_SCENARIOS)}")
        print(f"‚úÖ Software Engineer scenarios: {len(SOFTWARE_ENGINEER_SCENARIOS)}")
        
        # Test scenario structure
        scenario = ENGINEERING_MANAGER_SCENARIOS[0]
        required_keys = ['scenario', 'tickets', 'query', 'expected_insights']
        
        for key in required_keys:
            if key not in scenario:
                print(f"‚ùå Missing key '{key}' in scenario")
                return False
        
        print("‚úÖ Scenario structure validation passed")
        return True
        
    except Exception as e:
        print(f"‚ùå Dataset test failed: {e}")
        return False


def test_evaluation_setup():
    """Test that evaluation can be set up (without running it)."""
    print("\nüöÄ Testing Evaluation Setup...")
    
    try:
        # Test that we can create evaluator (may fail on API keys, that's OK)
        try:
            from langsmith_evaluator import SengyEvaluator
            evaluator = SengyEvaluator()
            print("‚úÖ Evaluator created successfully")
            has_evaluator = True
        except ValueError as e:
            if "Missing required configuration" in str(e):
                print("‚ö†Ô∏è  Evaluator needs API keys (expected)")
                has_evaluator = False
            else:
                raise
        
        # Test evaluation methods exist
        if has_evaluator:
            methods = ['create_dataset', 'run_evaluation', 'run_comprehensive_evaluation']
            for method in methods:
                if hasattr(evaluator, method):
                    print(f"‚úÖ Method '{method}' available")
                else:
                    print(f"‚ùå Method '{method}' missing")
                    return False
        
        print("‚úÖ Evaluation setup test passed")
        return True
        
    except Exception as e:
        print(f"‚ùå Evaluation setup test failed: {e}")
        return False


def main():
    """Run all tests."""
    print("üî¨ Sengy Evaluation Framework Test Suite")
    print("=" * 45)
    
    tests = [
        ("Import Tests", test_imports),
        ("Configuration Tests", test_configuration), 
        ("Dataset Tests", test_datasets),
        ("Evaluation Setup Tests", test_evaluation_setup)
    ]
    
    passed = 0
    total = len(tests)
    
    for name, test_func in tests:
        print(f"\nüìã {name}")
        print("-" * 30)
        if test_func():
            passed += 1
    
    print(f"\nüéØ Test Results: {passed}/{total} passed")
    
    if passed == total:
        print("üéâ All tests passed! Evaluation framework is ready.")
        
        print("\nüìù Next Steps:")
        print("1. Set API keys: OPENAI_API_KEY, LANGSMITH_API_KEY")
        print("2. Run: python eval/run_evaluation.py")
        print("3. View results in LangSmith dashboard")
        
        return True
    else:
        print("‚ùå Some tests failed. Please check the issues above.")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)